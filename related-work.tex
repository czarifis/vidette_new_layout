%\vspace*{-0.2cm}
\section{Related Work}
\label{section:related-work}


{\bf The FORWARD project.} The specific template language used in our project, follows heavily the syntax of templates in the FORWARD project \cite{SIGMOD2010,CIDR2011}. However, the semantics are different in order to fit the needs of notebook operation. Namely, the templates operate as scripts that produce nested Python instances for the units and fully redisplay them in each round. These scripts can appear in notebook coding blocks that the Jupyter crowd is comfortable with. Each coding block that utilizes these scripts essentially describes a view over data, that ultimately leads to a visualization. Unlike FORWARD, it is predetermined which units will be shown on each snapshot of the notebook. Thus, at the present state, it is not yet possible to create an interface that is tantamount to, say, a data cube exploration - which is something that would be possible in FORWARD. On the positive side, ViDeTTe (unlike FORWARD) does not require the analyst to understand the notebook as an MVVM application, whereas in each snapshot the framework visualizes a page with potentially different units and/or partially modified unit instances. 

Prior work related to \projname\ notebooks can be classified into the following categories:
\eat{
{\bf MVVM frameworks.} The idea of allowing application developers to use a declarative language to develop a web application has been the main selling point of many recent application frameworks, such as \angular\ \cite{angularjs}, \react\ \cite{react}, Ember \cite{ember} and others \cite{knockout, catel, mvvmcross, mvvmlight}.  These frameworks follow the MVVM (Model-View-ViewModel) paradigm, according to which a declarative template transforms a logical description of the data (referred to as \emph{model}) to a logical description of the visual state (also known as \emph{viewmodel}), which is then rendered as a visual page instance (also known as \emph{view}). Similarly to \projname, the declarative nature of the template increases developer productivity by simplifying the application specification \costas{development?} and automating the change propagation. However, as has been the case with many early declarative frameworks that automate tasks that were previously dealt with imperative code, existing MVVM frameworks still exhibit important inefficiencies. In particular, their change propagation algorithms are extremely inefficient, with a complexity that depends on the size of the entire model (i.e., the data) and/or the viewmodel (i.e., the template instance), instead of depending only on the number of the changes in the model and in the viewmodel, as is the case in \projname. Moreover, they either do not support JavaScript visualization libraries (as is the case with \react\ \cite{react}) or when they do (as is the case with \angular\ \cite{angularjs}) the process of wrapping existing libraries is very laborious. Finally, they limit the expressiveness of their template languages due to limitations of their change propagation algorithms. For a detailed comparison of \projname\ to MVVM frameworks, the reader is referred to Section \ref{section:angular-vs-forward}.

{\bf Incremental view maintenance approaches.} The concept of change propagation has been extensively studied in the database community in the form of incremental view maintenance (IVM). This has led to a substantial number of works both older  \cite{vm-blt-86, vm-bcl-89, vm-91, vm-qw-91, view-sigmod-93} and more recent \cite{dbtoaster-vldbj-2014, idivm-sigmod-2015} (see \cite{survey-deb-95,mv-survey-12} for surveys of IVM works). While inspired from IVM techniques, our change propagation algorithm extends existing IVM techniques in several important directions: it extends both the supported data model (adding support for semi-structured data and ordered collections) and the expressivity of the query language (adding support for user defined functions). Moreover, in contrast to IVM approaches which do not have any restrictions on the type of output diffs that they create, \projname's propagation algorithm has to respect the capabilities of the units (expressed in the form of supported unit instance diffs). For a detailed discussion of IVM works and their relationship to \projname, the reader is referred to Section \ref{section:ivm-discussion}.

{\bf Dynamic algorithms.} Change propagation has also been studied in the algorithms community in the form of dynamic algorithms. A dynamic algorithm efficiently updates the result of a computation, such as a clustering algorithm \cite{datta1993static}, a computational geometry algorithm \cite{chiang1992dynamic} or an algorithm that generates shortest path trees \cite{frigioni2000fully}. Although dynamic algorithms have been successfully used in many domains, they are tailored to a particular computation and cannot be easily extended to other classes of computations.

{\bf Incremental computation.} Change propagation has also been studied in the programming language community under the title of incremental computation. \yannisk{I am not sure yet what to say about these approaches. To get some background please see the commented out text by Costas and the corresponding wikipedia page on `incremental computing'}
}


%We mentioned the most closely related work, namely MVVM frameworks, in the previous sections of the paper. In section \ref{sec:mvvm-change-propagation}, we describe the fundamental limitations that prohibit such frameworks from operating in an efficient manner. Additionally, in section \ref{sec:ivm-in-databases} we describe the main focus of IVM techniques employed by database systems which is what powers our propagation algorithms. In this section, we briefly describe work that is more peripherally related to the concept of change propagation. This work has been conducted by researchers mainly in the fields of programming languages and algorithms.

%In the algorithms community, researchers have worked on designing dynamic algorithms that are capable of efficiently updating their output given dynamic changes on their input. Surveys that have been conducted in this area \cite{chiang1992dynamic} describe numerous approaches that have offered significant speedups in functions that resolve individual computational problems, especially when such functions operate on big data. These surveys also describe the significant effort that is usually associated with the development and implementation of such algorithms. Some of these algorithms took years of research to be developed and they are mostly oriented towards solving expensive domain specific problems (such as problems in computational geometry), while many such problems still remain open. Since these algorithms were carefully designed for solving individual computational problems they are not extensible in a way that would allow them to solve a wider range of problems. For that reason, they are cannot be used to perform change propagation in applications, mainly due to the diversity that such applications exhibit. 
 
%Elixir is a dynamic, functional language designed for building applications. Ecto is a domain specific language (Object-Relational mapping - ORM) that can be used in conjuction with Elixir to enable interaction with databases. One of the key concepts of Ecto (and other ORM libraries (such as doctrine, hibernate) is the notion of changesets. A Changeset -in this context- is essentially a construct that includes all the changes that will be applied to a back-end database as a result of user interaction. Ecto's API can be used to perform filtering, validation or describe extra constraints to a changeset before that is applied to a database. While this a fairly useful feature that assists in staging modifications and can potentially completely avoid or limit unnecessary or erroneous interactions with back-end databases by pushing down constraints to the application level, it cannot be used to efficiently propagate changes from the databases to the view (or the other way). Therefore, while both these tools and \projname utilize sets of changes their ultimate purpose is very different. 
 
%Meteor is a development platform that combines third party client-side libraries (such as AngularJS, ReactJS, Blaze, Cordova and others) and server-side libraries and databases (such as NodeJS, Connect, MongoDB and others) to enable the relatively effortless development of full-stack applications. While developer-friendliness of full stack applications is also one of the goals of \projname\, in \projname\ this is accomplished by using novel components that were designed from scratch with power and memory efficiency as a main priority. Meteor on the other hand, simply reuses existing software solutions across the stack.

%The goal of ZQL, which is the query language of Zenvisage, is to enable data analysts to extract visualizations that assist in identifying trends, patterns and insights from various sources in a declarative fashion. While ZQL manages to query databases and construct visualizations using minimal lines of code, the syntax of ZQL is more oriented towards data exploration experts therefore it is not very user friendly for application developers. Additionally, Zenvisage does not support unit wrapping, for that reason the user is limited to only utilizing a predefined set of available visualizations. Lastly, even if the underlying data used to construct a visualization change, Zenvisage will not automatically reflect those changes to the visualization (incrementally or otherwise). Instead the user has to reissue the ZQL query in order to reconstruct the update visualization.  

%In the programming languages community, researchers have developed techniques that achieve automatic incrementalization of programs. The main focus of these techniques \cite{ramalingam1993categorized} is the automatic translation of conventional programs into respective programs that can respond to dynamically changed data. Since these techniques provide tools or compiler techniques that automatically perform this translation, they manage to minimize the effort required for the implementation of such functions. Recent work on self adjusting computations also includes the use of specially designed high level languages (or the extension of existing languages with annotations \cite{hammer2009ceal, chen2012type}) used to express incremental computations that when combined with specifically developed compilers, can generate executables capable of efficiently handling mutations of input data. 

%A common denominator of the majority of these techniques is the fact that the underlying languages utilize a strong type system that enables the automatic distinction, (or in many cases the explicit manual distinction) between mutable and immutable input data. Leaving aside the fact that applications developers cannot necessarily predict which input data will be modified during the lifetime of an application, the fact that JavaScript is an untyped language can also be an obstacle, in using such techniques. It is unclear if these techniques could be used without forcefully introducing a type-system that application developers would have to adapt to. While, such a type-system might potentially assist in using some of these techniques to power incrementally maintained web applications, it would also steepen the learning curve of the respective framework since developers would have to familiarize themselves with the new type system. Lastly, it is unclear how such techniques can be used in a distributed architecture like the one described in Section \ref{sec:ivm-general}


\eat{
{\bf Declarative data-driven visualizations.} Finally, recent works in data-driven visualizations have suggested the use of declarative languages for the specification of visualizations. Notable examples of such works include Vega \cite{???} \yannisk{Is it Vega or Vega-lite?} and Zenvisage \cite{???}. While these works share the same observation with the current work that declarative languages can simplify the visualization specification process, they use declarative languages to accomplish different goals. Zenvisage's language is tuned towards easy data exploration, by allowing the user to create visualizations that reveal interesting patterns in the data. Vega... \yannisk{Costa, please add citations above and text on Vega}. In contrast, \projname\ leverages the declarative language to allow the specification of fully-fledged web applications that can use any of the existing JavaScript visualization libraries.
}

{\bf Grammar-based visualizations.} The concept of using a formal grammar to specify visualizations can be traced back to Wilkinson's ``The Grammar of Graphics" \cite{Wilkinson:2005:GG:1088896}. Since then multiple tools, such as Polaris \cite{stolte2002polaris} (later commercialized as Tableau), ggplot2 \cite{wickham2009ggplot2}, Vega \cite{satyanarayan2015vega, vega} and others \cite{ggvis, bostock2009protovis, d3} have adopted this approach, thus enabling the description of visualizations (as well as the user interaction with such visualizations \cite{Wu:2016:DAI:2939502.2939517, satyanarayan2016reactive}) in a terse and declarative manner. While the expressiveness of the employed grammars varies, their main focus is to effectively describe the specifics of the visualizations that will appear (for instance the dimensions of the chart, the colors, etc.), and not to describe data access or data processing and/or conversions. For that reason, such tools (due to their limited expressivity) do not provide constructs capable of retrieving data from diverse sources or performing arbitrary data transformations. Instead, the user has to provide imperative logic that performs these tasks manually and ultimately convert the dataset she wants to visualize into the format that is expected by the respective tool in order to generate the visualization.
 
{\bf Reactive Libraries.} Some of the aforementioned tools (for instance  ggplot2 \cite{wickham2009ggplot2}, Vega \cite{satyanarayan2015vega, vega, Altair, ipyvega} and ggvis \cite{ggvis}), do allow readers to interact with visualizations, however, they don't allow, data analysts to declaratively specify the wide range of side effects that \projname\ allows. Specifically, while these tools can create visualizations (and widgets) that depend on one another, they cannot declaratively describe more elaborate side-effects that include retrieving additional data, or issuing arbitrary computations (for instance, linear regression as was shown in this paper). They take as input the entire dataset that can be included in the produced visualizations throughout their life-cycle and they provide a declarative language with limited data processing (essentially describing a subset of the operations that can be expressed in SQL) and no data access capabilities. Additionally, these tools operate in isolation with respect to the rest of the notebook, which means that they cannot be used to generate co-dependent visualizations that appear across multiple notebook coding blocks. For those reasons they offer more limited data exploratory capabilities compared to \projname. 



\eat{
allow users to access data from various sources and most importantly they do not provide any data conversion modules capable of transforming the data to the required data type for the visualization. Therefore, the user has to manually convert the dataset she wants to visualize into a format that matches the expected schema and then simply evaluate it to generate the respective chart. 
}


{\bf Data exploration.}  Tools that use the human factor as an integral part of data exploration have gained in popularity in the past years. Such tools provide a simple to use way that describes both computation and visualizations, thus providing the required toolkit to effectively explore data and steer computation. Some of these tools enable this process by providing a declarative language capable of performing such tasks (such as Zenvisage \cite{DBLP:journals/corr/SiddiquiKLKP16} and Devise \cite{livny1997devise}), while others (such as Vizdom \cite{crotty2015vizdom}, DataHub \cite{Krishnan:2016:AID:2994509.2994514} and more\cite{derthick1997interactive,  bhardwaj2015collaborative, zoumpatianos2015rinse, DBLP:conf/icde/LiarouI14, Kamat:2016:TSI:2939502.2939514}) rely on the user interaction with a front-end, in order to compose the appropriate queries. As a result, the former tools assume a type of user with deep understanding in databases and query languages, while the latter offer a more user-friendly way of performing an analysis \eat{pertaining to a user that does not have to have such a strong background in the field }but at the same time allow a predetermined set of possible analyses. \projname\ notebooks bridge the gap between these two types of users; to a data analyst it provides constructs that allow rapid creation of notebooks for an arbitrary number of analyses, while to a non-technical user it provides the capability to interact with the produced visualization in order to further explore the underlying data.
